---
title: "Problem Set 2"
author: "Marc Ishak"
output:
  html_notebook:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Uni/2019T3/econ3203")

```

# Question 1
First, let's load our preferred packages and load the data into R
```{r}
library(readr)
library(ggplot2)
library(dplyr)
library(gvlma)
library(broom)
library(gridExtra)

df <- read_csv("data/pset2/dynamic.csv")  

```

## a

```{r}

regmod <- lm(static ~ dynamic, data = df)
regmod_sum <- summary(regmod)

regmod_sum

```
Hence the regression formula is static = `r regmod$coefficients[1]` + dynamic * `r regmod$coefficients[2]` + \(\epsilon\)

## b 

```{r, fig.width=8, fig.asp=1.5}
par(mfrow = c(3,2))
plot(regmod, which = 1:6)
```
```{r}
gvlma(regmod)

tidy(regmod, conf.int = T) %>% 
  ggplot(aes(term, estimate, color = term)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) 

```


##  c

```{r}
augment(regmod) %>%
  ggplot(aes(dynamic, static)) + 
    geom_point(aes(col = .cooksd)) +
    geom_smooth(method = "lm", se = 0.95) +
    labs(col = "Cooks Distance")

paste0("Number of Outliers: ", (augment(regmod) %>% 
  filter(.cooksd > qf(0.5,2,28)) %>% 
  nrow()))

```

Yes, as there's `r (augment(regmod) %>%  filter(.cooksd > qf(0.5,2,28)) %>% dim())[1]` point with a [cooks distance](https://en.wikipedia.org/wiki/Cook%27s_distance)  >  `r qf(0.5,2,28)`.


## d
Running a manual t test (testing the slope)

```{r}
t_val <- tidy(regmod)$statistic[2] 
# calculating it manually with tidy(regmod)$estimate[2]/tidy(regmod)$std.error[2] also works
dfree = dim(df)[1] - 2
print((1 - pt(q =  t_val, df = dfree))*2)
```

Alternatively you could just read it off the summary/get it from the tided model
```{r}
tidy(regmod_sum)$p.value[2] #rounding diffrence smh
```

Either case the probability that the slope = 0 (and hence the dynamic weight would be useless in predicting the static weight) is  `r tidy(regmod)$p.value[2]` which is smaller than 0.05. Hence we reject the null hypothesis and conclude that the dynamic weight is useful in predicting the static weight.

## Graph selections for data without outlier

We can clearly see that there exists an outlier in the original graph (cooks distance!!!). So here are the same calcs without the outlier. (This also is supposed to match more closely with my submitted stata version)

Calculating for the second graph (to match stata analysis)

```{r}

df2 <- augment(regmod) %>%
  filter(.cooksd < 1) %>% 
  semi_join(df, .)

regmod2 <- lm(static ~ dynamic, data = df2)
regmod_sum2 <- summary(regmod2)

regmod_sum
```

```{r, fig.width=8, fig.asp=1.5}
par(mfrow = c(3,2))
plot(regmod2, which = 1:6)
```
```{r}
gvlma(regmod2)

tidy(regmod2, conf.int = T) %>% 
  ggplot(aes(term, estimate, color = term)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) 

```

```{r}
augment(regmod2) %>%
  ggplot(aes(dynamic, static)) + 
    geom_point(aes(col = .cooksd)) +
    geom_smooth(method = "lm", se = 0.95) +
    labs(col = "Cooks Distance")

# Outlier count:

paste0("Number of Outliers: ", (augment(regmod2) %>% 
  filter(.cooksd > qf(0.5,2,27)) %>% 
  nrow()))


```



```{r}
t_val <- tidy(regmod2)$statistic[2] 
# calculating it manually with tidy(regmod)$estimate[2]/tidy(regmod)$std.error[2] also works
dfree = nrow(df2) - 2
print((1 - pt(q =  t_val, df = dfree))*2)
```

Alternatively you could just read it off the summary/get it from the tided model
```{r}
tidy(regmod_sum2)$p.value[2] #rounding diffrence smh
```

# Question 2

The formula for a confidance interal is:
\[
\hat{x} \pm  z_{1-\alpha/2}\frac{s}{\sqrt{n}}
\]

Hence as the size of n gets larger the "band" of the confidance interval shrinks. A confidance interval is used to provide a predicion of the population parameter (like the mean) and not the entire data set (like a prediction interval). You can see it in the graphs below:

```{r}


plots = list()
i = 1
for(n in c(10,50,100,200)){
  points = data.frame(x = numeric(n), y = numeric(n))
  points$x <- seq(1,10000, 10000/n)
  points$y <- sapply(points$x, function(q){q + rnorm(mean = 0, sd = 1000, n = 1)})
  plots[[i]] <- (ggplot(points,aes(x,y)) + geom_point(alpha = 0.3) + geom_smooth(method = "lm", se = 0.95, col = "red", fill = "blue"))
  i = i+1
}

v <- length(plots)
nCol <- floor(sqrt(v))
do.call("grid.arrange", c(plots, ncol=nCol))

```

